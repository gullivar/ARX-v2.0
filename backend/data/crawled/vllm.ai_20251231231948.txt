vLLM Menu Search Docs Documentation Blog Events Contact GitHub Community Theme Docs Blog Events Contact Community Search ‚åòJ The High-Throughput and Memory-Efficient inference and serving engine for LLMs Easy, fast, and cost-efficient LLM serving for everyone. Get Started Documentation Easy Deploy the widest range of open-source models on any hardware. Includes a drop-in OpenAI-compatible API for instant integration. Fast Maximize throughput with PagedAttention. Advanced scheduling and continuous batching ensure peak GPU utilization. Cost Efficient Slash inference costs by maximizing hardware efficiency. We make high-performance LLMs affordable and accessible to everyone. Quick Start Select your preferences and run the install command. Stable represents the most currently tested and supported version of vLLM. Nightly is available if you want the latest builds. üì¶ Requires Python 3.10+. Python 3.12+ recommended. ‚ö° We recommend uv for faster and more reliable installation. üîß For other platforms, see docs.vllm.ai üéâ See what's new in Build Stable Nightly Platform CUDA ROCm XPU CPU Package Python (uv) Python Docker CUDA Version CUDA 12.9 CUDA 13.0 Run this Command: uv pip install vllm üí° Compatible with all CUDA 12.x versions (12.0 - 12.9) Looking for older versions? Find which release contains a PR Sponsors vLLM is a community project. Our compute resources for development and testing are supported by the following organizations. Thank you for your support! Cash Donations a16z Sequoia Capital Skywork AI ZhenFund Compute Resources Alibaba Cloud AMD Anyscale AWS Crusoe Cloud Google Cloud IBM Intel Lambda Lab Nebius Novita AI NVIDIA Red Hat Roblox RunPod UC Berkeley Volcengine Slack Sponsor Anyscale ‚Äî Stars ‚Äî ‚≠ê ‚Äî Contributors ‚Äî üë• PyTorch Foundation We collect donation through GitHub and OpenCollective . We plan to use the fund to support the development, maintenance, and adoption of vLLM. Universal Compatibility One engine, endless possibilities. Run any model on any hardware. Hardware Unified API across platforms NVIDIA CUDA GPUs Popular AMD ROCm GPUs Ascend Huawei NPU AWS Neuron Inferentia & Trainium Google TPU Cloud TPU IBM Spyre AI Accelerator Intel Gaudi View all supported hardware Open Models Latest trending open-source models, optimized & production-ready DeepSeek DeepSeek R1 DeepSeek V3.2 Google Gemma 3 Gemma 3n Meta Llama 4 Scout Llama 4 Maverick Minimax MiniMax M2 MiniMax M2.1 MoonshotAI Kimi Linear Kimi K2 Mistral AI Mistral Large 3 Ministral 3 Qwen Qwen3 Qwen3 VL Z-AI GLM 4.6V GLM 4.7 View all supported models Everyone welcome! Got questions? We're here to help. Whether you're just getting started or debugging a complex deployment, our community is open to everyone. No question is too basic! Fast & friendly responses Active maintainers Join Slack Real-time help & discussions Visit Forum Searchable Q&A knowledge base GitHub Issues Bug reports & feature requests Resources Explore recipes, benchmarks, and roadmap Recipes Example notebooks and tutorials recipes.vllm.ai Performance Benchmarks and comparisons perf.vllm.ai Roadmap Project roadmap and milestones roadmap.vllm.ai Ecosystem Tools and libraries built around vLLM AIBrix ¬∑ AI infrastructure on K8s GuideLLM ¬∑ LLM performance evaluation LLM Compressor ¬∑ Model quantization Production Stack ¬∑ Production deployment Semantic Router ¬∑ Intelligent routing Speculators ¬∑ Speculative decoding vLLM Omni ¬∑ Omni-modality models ¬© 2025 vLLM ¬∑ All rights reserved. GitHub X LinkedIn Slack Discuss