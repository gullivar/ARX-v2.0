Trending Papers - Hugging Face Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up new Get trending papers in your email inbox once a day! Get trending papers in your email inbox! Subscribe Trending Papers by AK and the research community Daily Weekly Monthly Trending Papers Submitted by amael-apple Sharp Monocular View Synthesis in Less Than a Second SHARP synthesizes photorealistic views from a single image using a 3D Gaussian representation, achieving state-of-the-art results with rapid processing. Apple · Published on Dec 11, 2025 Upvote 21 GitHub 6.25k arXiv Page Submitted by amael-apple Sharp Monocular View Synthesis in Less Than a Second SHARP synthesizes photorealistic views from a single image using a 3D Gaussian representation, achieving state-of-the-art results with rapid processing. Apple · Dec 11, 2025 Upvote 21 GitHub 6.25k arXiv Page Submitted by jt-zhang TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times TurboDiffusion accelerates video generation by 100-200x using attention acceleration, step distillation, and quantization, while maintaining video quality. University of California, Berkeley · Published on Dec 18, 2025 Upvote 87 GitHub 2.9k arXiv Page Submitted by jt-zhang TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times TurboDiffusion accelerates video generation by 100-200x using attention acceleration, step distillation, and quantization, while maintaining video quality. University of California, Berkeley · Dec 18, 2025 Upvote 87 GitHub 2.9k arXiv Page Submitted by andito SmolDocling: An ultra-compact vision-language model for end-to-end
  multi-modal document conversion SmolDocling is a compact vision-language model that performs end-to-end document conversion with robust performance across various document types using 256M parameters and a new markup format. IBM Granite · Published on Mar 14, 2025 Upvote 122 GitHub 48.5k arXiv Page Submitted by andito SmolDocling: An ultra-compact vision-language model for end-to-end
  multi-modal document conversion SmolDocling is a compact vision-language model that performs end-to-end document conversion with robust performance across various document types using 256M parameters and a new markup format. IBM Granite · Mar 14, 2025 Upvote 122 GitHub 48.5k arXiv Page Self-Supervised Prompt Optimization A self-supervised framework optimizes prompts for both closed and open-ended tasks by evaluating LLM outputs without external references, reducing costs and required data. 9 authors · Published on Feb 7, 2025 Upvote 8 GitHub 62.1k arXiv Page Self-Supervised Prompt Optimization A self-supervised framework optimizes prompts for both closed and open-ended tasks by evaluating LLM outputs without external references, reducing costs and required data. 9 authors · Feb 7, 2025 Upvote 8 GitHub 62.1k arXiv Page Submitted by hao-li Agent READMEs: An Empirical Study of Context Files for Agentic Coding Agentic coding tools receive goals written in natural language as input, break them down into specific tasks, and write or execute the actual code with minimal human intervention. Central to this process are agent context files ("READMEs for agents") that provide persistent, project-level instructions. In this paper, we conduct the first large-scale empirical study of 2,303 agent context files from 1,925 repositories to characterize their structure, maintenance, and content. We find that these files are not static documentation but complex, difficult-to-read artifacts that evolve like configuration code, maintained through frequent, small additions. Our content analysis of 16 instruction types shows that developers prioritize functional context, such as build and run commands (62.3%), implementation details (69.9%), and architecture (67.7%). We also identify a significant gap: non-functional requirements like security (14.5%) and performance (14.5%) are rarely specified. These findings indicate that while developers use context files to make agents functional, they provide few guardrails to ensure that agent-written code is secure or performant, highlighting the need for improved tooling and practices. 11 authors · Published on Nov 17, 2025 Upvote 11 GitHub 13.6k arXiv Page Submitted by hao-li Agent READMEs: An Empirical Study of Context Files for Agentic Coding Agentic coding tools receive goals written in natural language as input, break them down into specific tasks, and write or execute the actual code with minimal human intervention. Central to this process are agent context files ("READMEs for agents") that provide persistent, project-level instructions. In this paper, we conduct the first large-scale empirical study of 2,303 agent context files from 1,925 repositories to characterize their structure, maintenance, and content. We find that these files are not static documentation but complex, difficult-to-read artifacts that evolve like configuration code, maintained through frequent, small additions. Our content analysis of 16 instruction types shows that developers prioritize functional context, such as build and run commands (62.3%), implementation details (69.9%), and architecture (67.7%). We also identify a significant gap: non-functional requirements like security (14.5%) and performance (14.5%) are rarely specified. These findings indicate that while developers use context files to make agents functional, they provide few guardrails to ensure that agent-written code is secure or performant, highlighting the need for improved tooling and practices. 11 authors · Nov 17, 2025 Upvote 11 GitHub 13.6k arXiv Page Submitted by akhaliq Efficient Memory Management for Large Language Model Serving with
  PagedAttention PagedAttention algorithm and vLLM system enhance the throughput of large language models by efficiently managing memory and reducing waste in the key-value cache. 9 authors · Published on Sep 12, 2023 Upvote 26 GitHub 66.6k arXiv Page Submitted by akhaliq Efficient Memory Management for Large Language Model Serving with
  PagedAttention PagedAttention algorithm and vLLM system enhance the throughput of large language models by efficiently managing memory and reducing waste in the key-value cache. 9 authors · Sep 12, 2023 Upvote 26 GitHub 66.6k arXiv Page Submitted by ethanchern LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation Real-time multimodal video generation via diffusion is enabled by an improved distillation approach with multimodal conditioning and optimized scheduling, reducing inference latency while maintaining quality for interactive systems. 7 authors · Published on Dec 29, 2025 Upvote 58 GitHub 115 arXiv Page Submitted by ethanchern LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation Real-time multimodal video generation via diffusion is enabled by an improved distillation approach with multimodal conditioning and optimized scheduling, reducing inference latency while maintaining quality for interactive systems. 7 authors · Dec 29, 2025 Upvote 58 GitHub 115 arXiv Page Submitted by taesiri PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact Vision-Language Model PaddleOCR-VL, a vision-language model combining NaViT-style dynamic resolution and ERNIE, achieves state-of-the-art performance in document parsing and element recognition with high efficiency. PaddlePaddle · Published on Oct 16, 2025 Upvote 109 GitHub 67.2k arXiv Page Submitted by taesiri PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact Vision-Language Model PaddleOCR-VL, a vision-language model combining NaViT-style dynamic resolution and ERNIE, achieves state-of-the-art performance in document parsing and element recognition with high efficiency. PaddlePaddle · Oct 16, 2025 Upvote 109 GitHub 67.2k arXiv Page Submitted by Yuanshi SpotEdit: Selective Region Editing in Diffusion Transformers Diffusion Transformer models have significantly advanced image editing by encoding conditional images and integrating them into transformer layers. However, most edits involve modifying only small regions, while current methods uniformly process and denoise all tokens at every timestep, causing redundant computation and potentially degrading unchanged areas. This raises a fundamental question: Is it truly necessary to regenerate every region during editing? To address this, we propose SpotEdit, a training-free diffusion editing framework that selectively updates only the modified regions. SpotEdit comprises two key components: SpotSelector identifies stable regions via perceptual similarity and skips their computation by reusing conditional image features; SpotFusion adaptively blends these features with edited tokens through a dynamic fusion mechanism, preserving contextual coherence and editing quality. By reducing unnecessary computation and maintaining high fidelity in unmodified areas, SpotEdit achieves efficient and precise image editing. National University of Singapore · Published on Dec 26, 2025 Upvote 32 GitHub 63 arXiv Page Submitted by Yuanshi SpotEdit: Selective Region Editing in Diffusion Transformers Diffusion Transformer models have significantly advanced image editing by encoding conditional images and integrating them into transformer layers. However, most edits involve modifying only small regions, while current methods uniformly process and denoise all tokens at every timestep, causing redundant computation and potentially degrading unchanged areas. This raises a fundamental question: Is it truly necessary to regenerate every region during editing? To address this, we propose SpotEdit, a training-free diffusion editing framework that selectively updates only the modified regions. SpotEdit comprises two key components: SpotSelector identifies stable regions via perceptual similarity and skips their computation by reusing conditional image features; SpotFusion adaptively blends these features with edited tokens through a dynamic fusion mechanism, preserving contextual coherence and editing quality. By reducing unnecessary computation and maintaining high fidelity in unmodified areas, SpotEdit achieves efficient and precise image editing. National University of Singapore · Dec 26, 2025 Upvote 32 GitHub 63 arXiv Page Submitted by Cxxs Decoupled DMD: CFG Augmentation as the Spear, Distribution Matching as the Shield The study reveals that in text-to-image generation, CFG Augmentation is the primary driver of few-step distillation in Distribution Matching Distillation (DMD), while the distribution matching term acts as a regularizer. Tongyi-MAI · Published on Nov 27, 2025 Upvote 28 GitHub 8.29k arXiv Page Submitted by Cxxs Decoupled DMD: CFG Augmentation as the Spear, Distribution Matching as the Shield The study reveals that in text-to-image generation, CFG Augmentation is the primary driver of few-step distillation in Distribution Matching Distillation (DMD), while the distribution matching term acts as a regularizer. Tongyi-MAI · Nov 27, 2025 Upvote 28 GitHub 8.29k arXiv Page Submitted by Paper99 Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer Z-Image, a 6B-parameter Scalable Single-Stream Diffusion Transformer (S3-DiT) model, achieves high-performance image generation with reduced computational cost, offering sub-second inference and compatibility with consumer hardware. Tongyi-MAI · Published on Nov 27, 2025 Upvote 217 GitHub 8.28k arXiv Page Submitted by Paper99 Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer Z-Image, a 6B-parameter Scalable Single-Stream Diffusion Transformer (S3-DiT) model, achieves high-performance image generation with reduced computational cost, offering sub-second inference and compatibility with consumer hardware. Tongyi-MAI · Nov 27, 2025 Upvote 217 GitHub 8.28k arXiv Page Submitted by kpzhang996 Yume-1.5: A Text-Controlled Interactive World Generation Model Recent approaches have demonstrated the promise of using diffusion models to generate interactive and explorable worlds. However, most of these methods face critical challenges such as excessively large parameter sizes, reliance on lengthy inference steps, and rapidly growing historical context, which severely limit real-time performance and lack text-controlled generation capabilities. To address these challenges, we propose \method, a novel framework designed to generate realistic, interactive, and continuous worlds from a single image or text prompt. \method achieves this through a carefully designed framework that supports keyboard-based exploration of the generated worlds. The framework comprises three core components: (1) a long-video generation framework integrating unified context compression with linear attention; (2) a real-time streaming acceleration strategy powered by bidirectional attention distillation and an enhanced text embedding scheme; (3) a text-controlled method for generating world events. We have provided the codebase in the supplementary material. 9 authors · Published on Dec 26, 2025 Upvote 51 GitHub 449 arXiv Page Submitted by kpzhang996 Yume-1.5: A Text-Controlled Interactive World Generation Model Recent approaches have demonstrated the promise of using diffusion models to generate interactive and explorable worlds. However, most of these methods face critical challenges such as excessively large parameter sizes, reliance on lengthy inference steps, and rapidly growing historical context, which severely limit real-time performance and lack text-controlled generation capabilities. To address these challenges, we propose \method, a novel framework designed to generate realistic, interactive, and continuous worlds from a single image or text prompt. \method achieves this through a carefully designed framework that supports keyboard-based exploration of the generated worlds. The framework comprises three core components: (1) a long-video generation framework integrating unified context compression with linear attention; (2) a real-time streaming acceleration strategy powered by bidirectional attention distillation and an enhanced text embedding scheme; (3) a text-controlled method for generating world events. We have provided the codebase in the supplementary material. 9 authors · Dec 26, 2025 Upvote 51 GitHub 449 arXiv Page Submitted by Rbin RAG-Anything: All-in-One RAG Framework RAG-Anything is a unified framework that enhances multimodal knowledge retrieval by integrating cross-modal relationships and semantic matching, outperforming existing methods on complex benchmarks. Data Intelligence Lab@HKU · Published on Oct 14, 2025 Upvote 53 GitHub 11.7k arXiv Page Submitted by Rbin RAG-Anything: All-in-One RAG Framework RAG-Anything is a unified framework that enhances multimodal knowledge retrieval by integrating cross-modal relationships and semantic matching, outperforming existing methods on complex benchmarks. Data Intelligence Lab@HKU · Oct 14, 2025 Upvote 53 GitHub 11.7k arXiv Page Submitted by taesiri MinerU2.5: A Decoupled Vision-Language Model for Efficient
  High-Resolution Document Parsing MinerU2.5, a 1.2B-parameter document parsing vision-language model, achieves state-of-the-art recognition accuracy with computational efficiency through a coarse-to-fine parsing strategy. 61 authors · Published on Sep 26, 2025 Upvote 139 GitHub 51.3k arXiv Page Submitted by taesiri MinerU2.5: A Decoupled Vision-Language Model for Efficient
  High-Resolution Document Parsing MinerU2.5, a 1.2B-parameter document parsing vision-language model, achieves state-of-the-art recognition accuracy with computational efficiency through a coarse-to-fine parsing strategy. 61 authors · Sep 26, 2025 Upvote 139 GitHub 51.3k arXiv Page Submitted by akhaliq FunAudioLLM: Voice Understanding and Generation Foundation Models for
  Natural Interaction Between Humans and LLMs FunAudioLLM enhances voice interactions by integrating SenseVoice for multilingual speech recognition, emotion detection, and audio event detection with CosyVoice for natural speech generation across languages, timbres, and styles. 1 authors · Published on Jul 4, 2024 Upvote 40 GitHub 18.7k arXiv Page Submitted by akhaliq FunAudioLLM: Voice Understanding and Generation Foundation Models for
  Natural Interaction Between Humans and LLMs FunAudioLLM enhances voice interactions by integrating SenseVoice for multilingual speech recognition, emotion detection, and audio event detection with CosyVoice for natural speech generation across languages, timbres, and styles. 1 authors · Jul 4, 2024 Upvote 40 GitHub 18.7k arXiv Page Submitted by taesiri StoryMem: Multi-shot Long Video Storytelling with Memory StoryMem enhances multi-shot video generation with cinematic quality and long-range consistency using a memory bank and pre-trained single-shot video diffusion models. ByteDance · Published on Dec 22, 2025 Upvote 17 GitHub 466 arXiv Page Submitted by taesiri StoryMem: Multi-shot Long Video Storytelling with Memory StoryMem enhances multi-shot video generation with cinematic quality and long-range consistency using a memory bank and pre-trained single-shot video diffusion models. ByteDance · Dec 22, 2025 Upvote 17 GitHub 466 arXiv Page Submitted by wanderkid MinerU: An Open-Source Solution for Precise Document Content Extraction MinerU is an open-source tool that enhances document content extraction using fine-tuned models and pre/postprocessing rules across diverse document types. 18 authors · Published on Sep 27, 2024 Upvote 36 GitHub 51.3k arXiv Page Submitted by wanderkid MinerU: An Open-Source Solution for Precise Document Content Extraction MinerU is an open-source tool that enhances document content extraction using fine-tuned models and pre/postprocessing rules across diverse document types. 18 authors · Sep 27, 2024 Upvote 36 GitHub 51.3k arXiv Page Submitted by akhaliq Very Large-Scale Multi-Agent Simulation in AgentScope Enhancements to the AgentScope platform improve scalability, efficiency, and ease of use for large-scale multi-agent simulations through distributed mechanisms, flexible environments, and user-friendly tools. 8 authors · Published on Jul 25, 2024 Upvote 35 GitHub 15k arXiv Page Submitted by akhaliq Very Large-Scale Multi-Agent Simulation in AgentScope Enhancements to the AgentScope platform improve scalability, efficiency, and ease of use for large-scale multi-agent simulations through distributed mechanisms, flexible environments, and user-friendly tools. 8 authors · Jul 25, 2024 Upvote 35 GitHub 15k arXiv Page Submitted by taesiri AgentScope 1.0: A Developer-Centric Framework for Building Agentic
  Applications AgentScope enhances agentic applications by providing flexible tool-based interactions, unified interfaces, and advanced infrastructure based on the ReAct paradigm, supporting efficient and safe development and deployment. 23 authors · Published on Aug 22, 2025 Upvote 53 GitHub 15k arXiv Page Submitted by taesiri AgentScope 1.0: A Developer-Centric Framework for Building Agentic
  Applications AgentScope enhances agentic applications by providing flexible tool-based interactions, unified interfaces, and advanced infrastructure based on the ReAct paradigm, supporting efficient and safe development and deployment. 23 authors · Aug 22, 2025 Upvote 53 GitHub 15k arXiv Page TradingAgents: Multi-Agents LLM Financial Trading Framework A multi-agent framework using large language models for stock trading simulates real-world trading firms, improving performance metrics like cumulative returns and Sharpe ratio. 4 authors · Published on Dec 28, 2024 Upvote 14 GitHub 27.3k arXiv Page TradingAgents: Multi-Agents LLM Financial Trading Framework A multi-agent framework using large language models for stock trading simulates real-world trading firms, improving performance metrics like cumulative returns and Sharpe ratio. 4 authors · Dec 28, 2024 Upvote 14 GitHub 27.3k arXiv Page Submitted by gatilin YOLO-Master: MOE-Accelerated with Specialized Transformers for Enhanced Real-time Detection Existing Real-Time Object Detection (RTOD) methods commonly adopt YOLO-like architectures for their favorable trade-off between accuracy and speed. However, these models rely on static dense computation that applies uniform processing to all inputs, misallocating representational capacity and computational resources such as over-allocating on trivial scenes while under-serving complex ones. This mismatch results in both computational redundancy and suboptimal detection performance. To overcome this limitation, we propose YOLO-Master, a novel YOLO-like framework that introduces instance-conditional adaptive computation for RTOD. This is achieved through a Efficient Sparse Mixture-of-Experts (ES-MoE) block that dynamically allocates computational resources to each input according to its scene complexity. At its core, a lightweight dynamic routing network guides expert specialization during training through a diversity enhancing objective, encouraging complementary expertise among experts. Additionally, the routing network adaptively learns to activate only the most relevant experts, thereby improving detection performance while minimizing computational overhead during inference. Comprehensive experiments on five large-scale benchmarks demonstrate the superiority of YOLO-Master. On MS COCO, our model achieves 42.4% AP with 1.62ms latency, outperforming YOLOv13-N by +0.8% mAP and 17.8% faster inference. Notably, the gains are most pronounced on challenging dense scenes, while the model preserves efficiency on typical inputs and maintains real-time inference speed. Code will be available. Tencent · Published on Dec 29, 2025 Upvote 11 GitHub 20 arXiv Page Submitted by gatilin YOLO-Master: MOE-Accelerated with Specialized Transformers for Enhanced Real-time Detection Existing Real-Time Object Detection (RTOD) methods commonly adopt YOLO-like architectures for their favorable trade-off between accuracy and speed. However, these models rely on static dense computation that applies uniform processing to all inputs, misallocating representational capacity and computational resources such as over-allocating on trivial scenes while under-serving complex ones. This mismatch results in both computational redundancy and suboptimal detection performance. To overcome this limitation, we propose YOLO-Master, a novel YOLO-like framework that introduces instance-conditional adaptive computation for RTOD. This is achieved through a Efficient Sparse Mixture-of-Experts (ES-MoE) block that dynamically allocates computational resources to each input according to its scene complexity. At its core, a lightweight dynamic routing network guides expert specialization during training through a diversity enhancing objective, encouraging complementary expertise among experts. Additionally, the routing network adaptively learns to activate only the most relevant experts, thereby improving detection performance while minimizing computational overhead during inference. Comprehensive experiments on five large-scale benchmarks demonstrate the superiority of YOLO-Master. On MS COCO, our model achieves 42.4% AP with 1.62ms latency, outperforming YOLOv13-N by +0.8% mAP and 17.8% faster inference. Notably, the gains are most pronounced on challenging dense scenes, while the model preserves efficiency on typical inputs and maintains real-time inference speed. Code will be available. Tencent · Dec 29, 2025 Upvote 11 GitHub 20 arXiv Page Submitted by unilm VibeVoice Technical Report VibeVoice synthesizes long-form multi-speaker speech using next-token diffusion and a highly efficient continuous speech tokenizer, achieving superior performance and fidelity. Microsoft Research · Published on Aug 26, 2025 Upvote 138 GitHub 19.3k arXiv Page Submitted by unilm VibeVoice Technical Report VibeVoice synthesizes long-form multi-speaker speech using next-token diffusion and a highly efficient continuous speech tokenizer, achieving superior performance and fidelity. Microsoft Research · Aug 26, 2025 Upvote 138 GitHub 19.3k arXiv Page IndexTTS: An Industrial-Level Controllable and Efficient Zero-Shot
  Text-To-Speech System IndexTTS, an enhanced text-to-speech system combining XTTS and Tortoise models, offers improved naturalness, enhanced voice cloning, and controllable usage through hybrid character-pinyin modeling and optimized vector quantization. 5 authors · Published on Feb 8, 2025 Upvote 6 GitHub 17.3k arXiv Page IndexTTS: An Industrial-Level Controllable and Efficient Zero-Shot
  Text-To-Speech System IndexTTS, an enhanced text-to-speech system combining XTTS and Tortoise models, offers improved naturalness, enhanced voice cloning, and controllable usage through hybrid character-pinyin modeling and optimized vector quantization. 5 authors · Feb 8, 2025 Upvote 6 GitHub 17.3k arXiv Page Submitted by taesiri PersonaLive! Expressive Portrait Image Animation for Live Streaming PersonaLive is a diffusion-based framework for real-time portrait animation that enhances speed and efficiency through multi-stage training, hybrid implicit signals, appearance distillation, and autoregressive micro-chunk streaming. GVC Lab at Great Bay University · Published on Dec 12, 2025 Upvote 32 GitHub 1.11k arXiv Page Submitted by taesiri PersonaLive! Expressive Portrait Image Animation for Live Streaming PersonaLive is a diffusion-based framework for real-time portrait animation that enhances speed and efficiency through multi-stage training, hybrid implicit signals, appearance distillation, and autoregressive micro-chunk streaming. GVC Lab at Great Bay University · Dec 12, 2025 Upvote 32 GitHub 1.11k arXiv Page LightRAG: Simple and Fast Retrieval-Augmented Generation LightRAG improves Retrieval-Augmented Generation by integrating graph structures for enhanced contextual awareness and efficient information retrieval, achieving better accuracy and response times. 5 authors · Published on Oct 8, 2024 Upvote 27 GitHub 26.9k arXiv Page LightRAG: Simple and Fast Retrieval-Augmented Generation LightRAG improves Retrieval-Augmented Generation by integrating graph structures for enhanced contextual awareness and efficient information retrieval, achieving better accuracy and response times. 5 authors · Oct 8, 2024 Upvote 27 GitHub 26.9k arXiv Page Submitted by akhaliq LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models LlamaFactory is a unified framework enabling efficient fine-tuning of large language models across various tasks using a web-based user interface. 5 authors · Published on Mar 20, 2024 Upvote 175 GitHub 64.7k arXiv Page Submitted by akhaliq LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models LlamaFactory is a unified framework enabling efficient fine-tuning of large language models across various tasks using a web-based user interface. 5 authors · Mar 20, 2024 Upvote 175 GitHub 64.7k arXiv Page Submitted by taesiri Step-DeepResearch Technical Report Step-DeepResearch, an end-to-end agent enhanced with a data synthesis strategy and progressive training, achieves expert-level capabilities in deep research scenarios, outperforming established models. StepFun · Published on Dec 23, 2025 Upvote 76 GitHub 338 arXiv Page Submitted by taesiri Step-DeepResearch Technical Report Step-DeepResearch, an end-to-end agent enhanced with a data synthesis strategy and progressive training, achieves expert-level capabilities in deep research scenarios, outperforming established models. StepFun · Dec 23, 2025 Upvote 76 GitHub 338 arXiv Page LeVo: High-Quality Song Generation with Multi-Preference Alignment LeVo, a framework combining an LM and a music codec, improves lyrics-to-song generation by parallelly modeling mixed and dual-track tokens, using transformer decoders, and employing direct preference optimization to enhance musicality and instruction following. 13 authors · Published on Jun 9, 2025 Upvote 6 GitHub 1.15k arXiv Page LeVo: High-Quality Song Generation with Multi-Preference Alignment LeVo, a framework combining an LM and a music codec, improves lyrics-to-song generation by parallelly modeling mixed and dual-track tokens, using transformer decoders, and employing direct preference optimization to enhance musicality and instruction following. 13 authors · Jun 9, 2025 Upvote 6 GitHub 1.15k arXiv Page Submitted by taesiri HunyuanVideo 1.5 Technical Report HunyuanVideo 1.5 is a lightweight video generation model with state-of-the-art visual quality and motion coherence, using a DiT architecture with SSTA and an efficient video super-resolution network. 81 authors · Published on Nov 24, 2025 Upvote 23 GitHub 2.35k arXiv Page Submitted by taesiri HunyuanVideo 1.5 Technical Report HunyuanVideo 1.5 is a lightweight video generation model with state-of-the-art visual quality and motion coherence, using a DiT architecture with SSTA and an efficient video super-resolution network. 81 authors · Nov 24, 2025 Upvote 23 GitHub 2.35k arXiv Page Submitted by zbhpku DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI DataFlow is an LLM-driven data preparation framework that enhances data quality and reproducibility for various tasks, improving LLM performance with automatically generated pipelines. Peking University · Published on Dec 18, 2025 Upvote 194 GitHub 1.96k arXiv Page Submitted by zbhpku DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI DataFlow is an LLM-driven data preparation framework that enhances data quality and reproducibility for various tasks, improving LLM performance with automatically generated pipelines. Peking University · Dec 18, 2025 Upvote 194 GitHub 1.96k arXiv Page AI-Trader: Benchmarking Autonomous Agents in Real-Time Financial Markets AI-Trader evaluates the performance of large language models in real-world financial markets, highlighting their limitations in trading and risk management. 6 authors · Published on Dec 1, 2025 Upvote 2 GitHub 10.5k arXiv Page AI-Trader: Benchmarking Autonomous Agents in Real-Time Financial Markets AI-Trader evaluates the performance of large language models in real-world financial markets, highlighting their limitations in trading and risk management. 6 authors · Dec 1, 2025 Upvote 2 GitHub 10.5k arXiv Page Submitted by Jeff-Wang GigaBrain-0: A World Model-Powered Vision-Language-Action Model GigaBrain-0, a VLA foundation model, uses world model-generated data to enhance cross-task generalization and policy robustness, improving real-world performance on complex manipulation tasks. GigaAI · Published on Oct 22, 2025 Upvote 49 GitHub 1.19k arXiv Page Submitted by Jeff-Wang GigaBrain-0: A World Model-Powered Vision-Language-Action Model GigaBrain-0, a VLA foundation model, uses world model-generated data to enhance cross-task generalization and policy robustness, improving real-world performance on complex manipulation tasks. GigaAI · Oct 22, 2025 Upvote 49 GitHub 1.19k arXiv Page Submitted by Jiaqi-hkust Robust-R1: Degradation-Aware Reasoning for Robust Visual Understanding A novel framework, Robust-R1, enhances multimodal large language models' robustness to visual degradations through explicit modeling, supervised fine-tuning, reward-driven alignment, and dynamic reasoning depth scaling, achieving state-of-the-art performance on real-world degradation benchmarks. 10 authors · Published on Dec 19, 2025 Upvote 64 GitHub 381 arXiv Page Submitted by Jiaqi-hkust Robust-R1: Degradation-Aware Reasoning for Robust Visual Understanding A novel framework, Robust-R1, enhances multimodal large language models' robustness to visual degradations through explicit modeling, supervised fine-tuning, reward-driven alignment, and dynamic reasoning depth scaling, achieving state-of-the-art performance on real-world degradation benchmarks. 10 authors · Dec 19, 2025 Upvote 64 GitHub 381 arXiv Page olmOCR: Unlocking Trillions of Tokens in PDFs with Vision Language
  Models olmOCR is an open-source toolkit using a fine-tuned vision language model to process PDFs into clean text while preserving structure, optimized for large-scale batch processing. 9 authors · Published on Feb 25, 2025 Upvote 9 GitHub 16.5k arXiv Page olmOCR: Unlocking Trillions of Tokens in PDFs with Vision Language
  Models olmOCR is an open-source toolkit using a fine-tuned vision language model to process PDFs into clean text while preserving structure, optimized for large-scale batch processing. 9 authors · Feb 25, 2025 Upvote 9 GitHub 16.5k arXiv Page Submitted by taesiri SAM 3: Segment Anything with Concepts Segment Anything Model 3 achieves state-of-the-art performance in promptable concept segmentation and tracking by leveraging a unified model architecture with decoupled recognition and localization. AI at Meta · Published on Nov 20, 2025 Upvote 123 GitHub 6.65k arXiv Page Submitted by taesiri SAM 3: Segment Anything with Concepts Segment Anything Model 3 achieves state-of-the-art performance in promptable concept segmentation and tracking by leveraging a unified model architecture with decoupled recognition and localization. AI at Meta · Nov 20, 2025 Upvote 123 GitHub 6.65k arXiv Page Submitted by akhaliq Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory Mem0, a memory-centric architecture with graph-based memory, enhances long-term conversational coherence in LLMs by efficiently extracting, consolidating, and retrieving information, outperforming existing memory systems in terms of accuracy and computational efficiency. 5 authors · Published on Apr 28, 2025 Upvote 36 GitHub 44.8k arXiv Page Submitted by akhaliq Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory Mem0, a memory-centric architecture with graph-based memory, enhances long-term conversational coherence in LLMs by efficiently extracting, consolidating, and retrieving information, outperforming existing memory systems in terms of accuracy and computational efficiency. 5 authors · Apr 28, 2025 Upvote 36 GitHub 44.8k arXiv Page Zep: A Temporal Knowledge Graph Architecture for Agent Memory Zep, a memory layer service, outperforms MemGPT in the DMR benchmark and LongMemEval by excelling in dynamic knowledge integration and temporal reasoning, critical for enterprise use cases. 5 authors · Published on Jan 20, 2025 Upvote 8 GitHub 21.5k arXiv Page Zep: A Temporal Knowledge Graph Architecture for Agent Memory Zep, a memory layer service, outperforms MemGPT in the DMR benchmark and LongMemEval by excelling in dynamic knowledge integration and temporal reasoning, critical for enterprise use cases. 5 authors · Jan 20, 2025 Upvote 8 GitHub 21.5k arXiv Page Submitted by Weiyun1025 InternVL3: Exploring Advanced Training and Test-Time Recipes for
  Open-Source Multimodal Models InternVL3 is a multimodal pre-trained language model that jointly learns from both multimodal data and text, improving performance and scalability through advanced techniques and setting a new state-of-the-art in multimodal tasks. 47 authors · Published on Apr 14, 2025 Upvote 306 arXiv Page Submitted by Weiyun1025 InternVL3: Exploring Advanced Training and Test-Time Recipes for
  Open-Source Multimodal Models InternVL3 is a multimodal pre-trained language model that jointly learns from both multimodal data and text, improving performance and scalability through advanced techniques and setting a new state-of-the-art in multimodal tasks. 47 authors · Apr 14, 2025 Upvote 306 arXiv Page Submitted by zhongwenxu Single-stream Policy Optimization Single-stream Policy Optimization (SPO) improves policy-gradient training for Large Language Models by eliminating group-based issues and providing a stable, low-variance learning signal, leading to better performance and efficiency. Tencent · Published on Sep 16, 2025 Upvote 34 GitHub 17.9k arXiv Page Submitted by zhongwenxu Single-stream Policy Optimization Single-stream Policy Optimization (SPO) improves policy-gradient training for Large Language Models by eliminating group-based issues and providing a stable, low-variance learning signal, leading to better performance and efficiency. Tencent · Sep 16, 2025 Upvote 34 GitHub 17.9k arXiv Page Submitted by Sansa Dream-VL & Dream-VLA: Open Vision-Language and Vision-Language-Action Models with Diffusion Language Model Backbone Diffusion-based vision-language models and action frameworks demonstrate superior performance in visual planning and robotic control tasks compared to autoregressive baselines. The University of Hong Kong · Published on Dec 27, 2025 Upvote 32 GitHub 47 arXiv Page Submitted by Sansa Dream-VL & Dream-VLA: Open Vision-Language and Vision-Language-Action Models with Diffusion Language Model Backbone Diffusion-based vision-language models and action frameworks demonstrate superior performance in visual planning and robotic control tasks compared to autoregressive baselines. The University of Hong Kong · Dec 27, 2025 Upvote 32 GitHub 47 arXiv Page Submitted by wymanCV Stable Video Infinity: Infinite-Length Video Generation with Error
  Recycling Stable Video Infinity generates infinite-length videos with high temporal consistency and controllable storylines by using Error-Recycling Fine-Tuning on the Diffusion Transformer. EPFL VITA Lab · Published on Oct 10, 2025 Upvote 17 GitHub 958 arXiv Page Submitted by wymanCV Stable Video Infinity: Infinite-Length Video Generation with Error
  Recycling Stable Video Infinity generates infinite-length videos with high temporal consistency and controllable storylines by using Error-Recycling Fine-Tuning on the Diffusion Transformer. EPFL VITA Lab · Oct 10, 2025 Upvote 17 GitHub 958 arXiv Page Submitted by dyyyyyyyy FAPO: Flawed-Aware Policy Optimization for Efficient and Reliable
  Reasoning Flawed-Aware Policy Optimization (FAPO) enhances reinforcement learning with verifiable rewards by penalizing flawed-positive rollouts, improving reasoning capability and training stability in large language models. 6 authors · Published on Oct 26, 2025 Upvote 11 GitHub 17.9k arXiv Page Submitted by dyyyyyyyy FAPO: Flawed-Aware Policy Optimization for Efficient and Reliable
  Reasoning Flawed-Aware Policy Optimization (FAPO) enhances reinforcement learning with verifiable rewards by penalizing flawed-positive rollouts, improving reasoning capability and training stability in large language models. 6 authors · Oct 26, 2025 Upvote 11 GitHub 17.9k arXiv Page OmniFlatten: An End-to-end GPT Model for Seamless Voice Conversation A novel GPT-based model, OmniFlatten, enables real-time natural full-duplex spoken dialogue through a multi-stage post-training technique that integrates speech and text without altering the original model's architecture. 9 authors · Published on Oct 23, 2024 Upvote 7 GitHub 51.6k arXiv Page OmniFlatten: An End-to-end GPT Model for Seamless Voice Conversation A novel GPT-based model, OmniFlatten, enables real-time natural full-duplex spoken dialogue through a multi-stage post-training technique that integrates speech and text without altering the original model's architecture. 9 authors · Oct 23, 2024 Upvote 7 GitHub 51.6k arXiv Page Submitted by taesiri SAM Audio: Segment Anything in Audio SAM Audio, a diffusion transformer-based foundation model, achieves superior performance in general audio separation using unified text, visual, and temporal span prompts across various audio types. AI at Meta · Published on Dec 19, 2025 Upvote 21 GitHub 2.67k arXiv Page Submitted by taesiri SAM Audio: Segment Anything in Audio SAM Audio, a diffusion transformer-based foundation model, achieves superior performance in general audio separation using unified text, visual, and temporal span prompts across various audio types. AI at Meta · Dec 19, 2025 Upvote 21 GitHub 2.67k arXiv Page Submitted by daixufang Agent Lightning: Train ANY AI Agents with Reinforcement Learning Agent Lightning is a flexible RL framework for training LLMs in various agents, using a hierarchical RL algorithm and decoupling execution from training to handle complex interactions. 8 authors · Published on Aug 5, 2025 Upvote 121 GitHub 9.98k arXiv Page Submitted by daixufang Agent Lightning: Train ANY AI Agents with Reinforcement Learning Agent Lightning is a flexible RL framework for training LLMs in various agents, using a hierarchical RL algorithm and decoupling execution from training to handle complex interactions. 8 authors · Aug 5, 2025 Upvote 121 GitHub 9.98k arXiv Page Submitted by taesiri DeepCode: Open Agentic Coding DeepCode, a fully autonomous framework, addresses the challenges of document-to-codebase synthesis by optimizing information flow through source compression, structured indexing, knowledge injection, and error correction, achieving state-of-the-art performance and surpassing human experts. 5 authors · Published on Dec 8, 2025 Upvote 31 GitHub 13.2k arXiv Page Submitted by taesiri DeepCode: Open Agentic Coding DeepCode, a fully autonomous framework, addresses the challenges of document-to-codebase synthesis by optimizing information flow through source compression, structured indexing, knowledge injection, and error correction, achieving state-of-the-art performance and surpassing human experts. 5 authors · Dec 8, 2025 Upvote 31 GitHub 13.2k arXiv Page Submitted by JDihlmann 3D-RE-GEN: 3D Reconstruction of Indoor Scenes with a Generative Framework 3D-RE-GEN reconstructs single images into modifiable 3D textured mesh scenes with comprehensive backgrounds, achieving top performance through compositional generation and scene optimization. 3 authors · Published on Dec 19, 2025 Upvote 11 GitHub 220 arXiv Page Submitted by JDihlmann 3D-RE-GEN: 3D Reconstruction of Indoor Scenes with a Generative Framework 3D-RE-GEN reconstructs single images into modifiable 3D textured mesh scenes with comprehensive backgrounds, achieving top performance through compositional generation and scene optimization. 3 authors · Dec 19, 2025 Upvote 11 GitHub 220 arXiv Page Submitted by Andrew613 UniPercept: Towards Unified Perceptual-Level Image Understanding across Aesthetics, Quality, Structure, and Texture Multimodal large language models (MLLMs) have achieved remarkable progress in visual understanding tasks such as visual grounding, segmentation, and captioning. However, their ability to perceive perceptual-level image features remains limited. In this work, we present UniPercept-Bench, a unified framework for perceptual-level image understanding across three key domains: Aesthetics, Quality, Structure and Texture. We establish a hierarchical definition system and construct large-scale datasets to evaluate perceptual-level image understanding. Based on this foundation, we develop a strong baseline UniPercept trained via Domain-Adaptive Pre-Training and Task-Aligned RL, enabling robust generalization across both Visual Rating (VR) and Visual Question Answering (VQA) tasks. UniPercept outperforms existing MLLMs on perceptual-level image understanding and can serve as a plug-and-play reward model for text-to-image generation. This work defines Perceptual-Level Image Understanding in the era of MLLMs and, through the introduction of a comprehensive benchmark together with a strong baseline, provides a solid foundation for advancing perceptual-level multimodal image understanding. 15 authors · Published on Dec 25, 2025 Upvote 23 GitHub 36 arXiv Page Submitted by Andrew613 UniPercept: Towards Unified Perceptual-Level Image Understanding across Aesthetics, Quality, Structure, and Texture Multimodal large language models (MLLMs) have achieved remarkable progress in visual understanding tasks such as visual grounding, segmentation, and captioning. However, their ability to perceive perceptual-level image features remains limited. In this work, we present UniPercept-Bench, a unified framework for perceptual-level image understanding across three key domains: Aesthetics, Quality, Structure and Texture. We establish a hierarchical definition system and construct large-scale datasets to evaluate perceptual-level image understanding. Based on this foundation, we develop a strong baseline UniPercept trained via Domain-Adaptive Pre-Training and Task-Aligned RL, enabling robust generalization across both Visual Rating (VR) and Visual Question Answering (VQA) tasks. UniPercept outperforms existing MLLMs on perceptual-level image understanding and can serve as a plug-and-play reward model for text-to-image generation. This work defines Perceptual-Level Image Understanding in the era of MLLMs and, through the introduction of a comprehensive benchmark together with a strong baseline, provides a solid foundation for advancing perceptual-level multimodal image understanding. 15 authors · Dec 25, 2025 Upvote 23 GitHub 36 arXiv Page Submitted by myyzzzoooo InsertAnywhere: Bridging 4D Scene Geometry and Diffusion Models for Realistic Video Object Insertion InsertAnywhere framework enhances video object insertion by generating geometrically consistent and visually coherent scenarios through 4D aware mask generation and diffusion-based synthesis. KAIST AI · Published on Dec 19, 2025 Upvote 91 GitHub 48 arXiv Page Submitted by myyzzzoooo InsertAnywhere: Bridging 4D Scene Geometry and Diffusion Models for Realistic Video Object Insertion InsertAnywhere framework enhances video object insertion by generating geometrically consistent and visually coherent scenarios through 4D aware mask generation and diffusion-based synthesis. KAIST AI · Dec 19, 2025 Upvote 91 GitHub 48 arXiv Page Submitted by X-iZhang A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm
  Bridging Foundation Models and Lifelong Agentic Systems A survey of self-evolving AI agents that adapt to dynamic environments through automatic enhancement based on interaction data and feedback. 15 authors · Published on Aug 10, 2025 Upvote 98 GitHub 1.61k arXiv Page Submitted by X-iZhang A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm
  Bridging Foundation Models and Lifelong Agentic Systems A survey of self-evolving AI agents that adapt to dynamic environments through automatic enhancement based on interaction data and feedback. 15 authors · Aug 10, 2025 Upvote 98 GitHub 1.61k arXiv Page System theme Company TOS Privacy About Careers Website Models Datasets Spaces Pricing Docs